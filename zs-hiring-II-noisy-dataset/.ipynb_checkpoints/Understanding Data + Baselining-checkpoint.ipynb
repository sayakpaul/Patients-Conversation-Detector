{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f16b85e568f49988b8a855ea529ccc4353e0593e"
   },
   "source": [
    "## Identify online patient conversations\n",
    "\n",
    "ZS Data Science team collaborates with the Social Listening team for automating the process of gaining insights from social media conversations.\n",
    "\n",
    "The Social Listening team has to manually validate heart failure related conversations fetched from the social listening tool which scans twitter, Facebook, forums, blogs etc.  Such conversations are posted by multiple stakeholders like patients, doctors, media houses, general public, etc. The team needs to identify the patient conversations, so as to dig deeper into them and identify the patient needs. The data science team wants to automate this process by building intelligent algorithms to predict patient conversations.\n",
    "\n",
    "Build an Intelligent pipeline that can segregate patient conversations from the rest of the group given historically tagged patient data.\n",
    "\n",
    "> You are expected to build an algorithm where they can ingest the social data and get the patient tags - 1 if patient and 0 if not a patient.\n",
    "\n",
    "Please find the dataset at [http://hck.re/pnsNa4](http://hck.re/pnsNa4).\n",
    "\n",
    "Description of attributes in dataset is given below - \n",
    "\n",
    "<img src = \"https://i.ibb.co/GHX5SZf/Capture.jpg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Dependecies\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../dataset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "523c745c1c0244a151fab2039fdd5c6ff2d9a149"
   },
   "outputs": [],
   "source": [
    "# Specify the encoding exlicitly as the datasets are not encoded in UTF-8 \n",
    "train_df = pd.read_csv('../dataset/train.csv', encoding = \"ISO-8859-1\")\n",
    "test_df = pd.read_csv('../dataset/test.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the dimensions of the train and test sets respectively?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1157, 9), (571, 10))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isn't it bit strange? The train set should ideally be higher than the test set in terms of dimensionality . Let's find out why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Source', 'Host', 'Link', 'Date(ET)', 'Time(ET)', 'time(GMT)', 'Title',\n",
       "       'TRANS_CONV_TEXT', 'Patient_Tag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Index', 'Source', 'Host', 'Link', 'Date(ET)', 'Time(ET)', 'time(GMT)',\n",
       "       'Title', 'TRANS_CONV_TEXT', 'Unnamed: 9'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it turns out that the test set has two columns `Index` and `Unnamed: 9` which are not present in the train set. The label here is the `Patient_Tag` column which is not present in the test set which is normal. \n",
    "\n",
    "Now, let's see what are these two columns `Index` and `Unnamed: 9` conveying? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index Unnamed: 9\n",
       "0      1        NaN\n",
       "1      2        NaN\n",
       "2      3        NaN\n",
       "3      4        NaN\n",
       "4      5        NaN\n",
       "5      6        NaN\n",
       "6      7        NaN\n",
       "7      8        NaN\n",
       "8      9        NaN\n",
       "9     10        NaN"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[['Index','Unnamed: 9']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like `Index` column is just denoting the indices of the samples explicitly and `Unnamed: 9` column is there erroneously. So, dropping them won't affect the performance of the final model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.drop(columns = ['Index', 'Unnamed: 9'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Source', 'Host', 'Link', 'Date(ET)', 'Time(ET)', 'time(GMT)', 'Title',\n",
       "       'TRANS_CONV_TEXT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "95dbcabe8ec6ef41d635f84c1318ed6c22e2a0f4"
   },
   "source": [
    "Let's now take a look at the dataset itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "9a118f41918fc4578655d52fbd9e7d7469161f35"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Host</th>\n",
       "      <th>Link</th>\n",
       "      <th>Date(ET)</th>\n",
       "      <th>Time(ET)</th>\n",
       "      <th>time(GMT)</th>\n",
       "      <th>Title</th>\n",
       "      <th>TRANS_CONV_TEXT</th>\n",
       "      <th>Patient_Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>FORUMS</td>\n",
       "      <td>www.healthrevelations.com</td>\n",
       "      <td>http://www.healthrevelations.com/2016/04/04/ar...</td>\n",
       "      <td>4/3/2016</td>\n",
       "      <td>21:00:00</td>\n",
       "      <td>4/4/2016 6:30</td>\n",
       "      <td>Arthritis painkillers pack serious heart risks</td>\n",
       "      <td>Arthritis painkillers pack serious heart risks...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>FORUMS</td>\n",
       "      <td>community.babycenter.com</td>\n",
       "      <td>http://community.babycenter.com/post/a63274957...</td>\n",
       "      <td>7/10/2016</td>\n",
       "      <td>13:24:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Update: I have been taking it by pill also thi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>BLOG</td>\n",
       "      <td>http://healthandfitness1blog.blogspot.com</td>\n",
       "      <td>http://healthandfitness1blog.blogspot.com/2016...</td>\n",
       "      <td>3/4/2016</td>\n",
       "      <td>6:23:00</td>\n",
       "      <td>3/4/2016 16:53</td>\n",
       "      <td>High coffee consumption may lower MS risk</td>\n",
       "      <td>Caffeine?s neuroprotective and anti-inflammato...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>FORUMS</td>\n",
       "      <td>www.xboxhacker.org</td>\n",
       "      <td>http://www.xboxhacker.org/index.php?topic=7219...</td>\n",
       "      <td>6/18/2016</td>\n",
       "      <td>4:48:00</td>\n",
       "      <td>6/18/2016 14:18</td>\n",
       "      <td>Dosing can be paced as a side effect which ord...</td>\n",
       "      <td>Hematoma is a shot which scold always a profes...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>FORUMS</td>\n",
       "      <td>allnurses.com</td>\n",
       "      <td>http://allnurses.com/nurse-colleague-patient/w...</td>\n",
       "      <td>7/29/2016</td>\n",
       "      <td>17:20:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I just got report the other day from a newer n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>FORUMS</td>\n",
       "      <td>forums.hardwarezone.com.sg</td>\n",
       "      <td>http://forums.hardwarezone.com.sg/money-mind-2...</td>\n",
       "      <td>4/29/2016</td>\n",
       "      <td>22:18:00</td>\n",
       "      <td>4/30/2016 7:48</td>\n",
       "      <td>AIA shield Plan. Help me understand</td>\n",
       "      <td>Yes ur understanding is correct. For yr father...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>FORUMS</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>https://www.reddit.com/r/childfree/comments/4v...</td>\n",
       "      <td>7/28/2016</td>\n",
       "      <td>23:31:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My family has a really bad genetic line (histo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>FORUMS</td>\n",
       "      <td>community.diabetes.org</td>\n",
       "      <td>http://community.diabetes.org/t5/Adults-Living...</td>\n",
       "      <td>7/6/2016</td>\n",
       "      <td>13:05:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I have worn a Holter monitor. It records your ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>BLOG</td>\n",
       "      <td>http://sciencedaily.com/news</td>\n",
       "      <td>https://www.sciencedaily.com/releases/2016/04/...</td>\n",
       "      <td>4/4/2016</td>\n",
       "      <td>18:10:00</td>\n",
       "      <td>4/5/2016 3:40</td>\n",
       "      <td>New device for heart failure patients fails to...</td>\n",
       "      <td>A new implantable medical device intended to h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>FORUMS</td>\n",
       "      <td>www.lse.co.uk</td>\n",
       "      <td>http://www.lse.co.uk/ShareChat.asp?ShareTicker...</td>\n",
       "      <td>6/19/2016</td>\n",
       "      <td>8:00:00</td>\n",
       "      <td>6/19/2016 17:30</td>\n",
       "      <td>Cloudtag CTAG</td>\n",
       "      <td>co-founded this company http://www.impulse-dyn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Source                                       Host  \\\n",
       "160   FORUMS                  www.healthrevelations.com   \n",
       "167   FORUMS                   community.babycenter.com   \n",
       "1030    BLOG  http://healthandfitness1blog.blogspot.com   \n",
       "573   FORUMS                         www.xboxhacker.org   \n",
       "416   FORUMS                              allnurses.com   \n",
       "975   FORUMS                 forums.hardwarezone.com.sg   \n",
       "488   FORUMS                                 reddit.com   \n",
       "619   FORUMS                     community.diabetes.org   \n",
       "1093    BLOG               http://sciencedaily.com/news   \n",
       "657   FORUMS                              www.lse.co.uk   \n",
       "\n",
       "                                                   Link   Date(ET)  Time(ET)  \\\n",
       "160   http://www.healthrevelations.com/2016/04/04/ar...   4/3/2016  21:00:00   \n",
       "167   http://community.babycenter.com/post/a63274957...  7/10/2016  13:24:00   \n",
       "1030  http://healthandfitness1blog.blogspot.com/2016...   3/4/2016   6:23:00   \n",
       "573   http://www.xboxhacker.org/index.php?topic=7219...  6/18/2016   4:48:00   \n",
       "416   http://allnurses.com/nurse-colleague-patient/w...  7/29/2016  17:20:00   \n",
       "975   http://forums.hardwarezone.com.sg/money-mind-2...  4/29/2016  22:18:00   \n",
       "488   https://www.reddit.com/r/childfree/comments/4v...  7/28/2016  23:31:00   \n",
       "619   http://community.diabetes.org/t5/Adults-Living...   7/6/2016  13:05:00   \n",
       "1093  https://www.sciencedaily.com/releases/2016/04/...   4/4/2016  18:10:00   \n",
       "657   http://www.lse.co.uk/ShareChat.asp?ShareTicker...  6/19/2016   8:00:00   \n",
       "\n",
       "            time(GMT)                                              Title  \\\n",
       "160     4/4/2016 6:30     Arthritis painkillers pack serious heart risks   \n",
       "167               NaN                                                NaN   \n",
       "1030   3/4/2016 16:53          High coffee consumption may lower MS risk   \n",
       "573   6/18/2016 14:18  Dosing can be paced as a side effect which ord...   \n",
       "416               NaN                                                NaN   \n",
       "975    4/30/2016 7:48                AIA shield Plan. Help me understand   \n",
       "488               NaN                                                NaN   \n",
       "619               NaN                                                NaN   \n",
       "1093    4/5/2016 3:40  New device for heart failure patients fails to...   \n",
       "657   6/19/2016 17:30                                      Cloudtag CTAG   \n",
       "\n",
       "                                        TRANS_CONV_TEXT  Patient_Tag  \n",
       "160   Arthritis painkillers pack serious heart risks...            0  \n",
       "167   Update: I have been taking it by pill also thi...            1  \n",
       "1030  Caffeine?s neuroprotective and anti-inflammato...            0  \n",
       "573   Hematoma is a shot which scold always a profes...            0  \n",
       "416   I just got report the other day from a newer n...            1  \n",
       "975   Yes ur understanding is correct. For yr father...            0  \n",
       "488   My family has a really bad genetic line (histo...            1  \n",
       "619   I have worn a Holter monitor. It records your ...            1  \n",
       "1093  A new implantable medical device intended to h...            0  \n",
       "657   co-founded this company http://www.impulse-dyn...            0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly sample 10 rows from the train set\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Host</th>\n",
       "      <th>Link</th>\n",
       "      <th>Date(ET)</th>\n",
       "      <th>Time(ET)</th>\n",
       "      <th>time(GMT)</th>\n",
       "      <th>Title</th>\n",
       "      <th>TRANS_CONV_TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>FORUMS</td>\n",
       "      <td>healthunlocked.com</td>\n",
       "      <td>https://healthunlocked.com/afassociation/posts...</td>\n",
       "      <td>5/7/2016</td>\n",
       "      <td>0.388888889</td>\n",
       "      <td>42497.78472</td>\n",
       "      <td>HealthUnlocked | The social network for health</td>\n",
       "      <td>Hi all I know that there have been lots of pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>FORUMS</td>\n",
       "      <td>www.alsearsmd.com</td>\n",
       "      <td>http://www.alsearsmd.com/2016/04/better-than-a...</td>\n",
       "      <td>4/1/2016</td>\n",
       "      <td>10:57:00</td>\n",
       "      <td>4/1/2016 20:27</td>\n",
       "      <td>Better than Aspirin for Your Heart</td>\n",
       "      <td>Health Articles Better than Aspirin for Your H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>YOUTUBE</td>\n",
       "      <td>http://www.youtube.com</td>\n",
       "      <td>http://youtube.com/watch?v=Nx8kpdUkqME</td>\n",
       "      <td>6/21/2016</td>\n",
       "      <td>16:46:41</td>\n",
       "      <td>6/22/2016 2:16</td>\n",
       "      <td>Heart Failure: Palliative Approaches to Care O...</td>\n",
       "      <td>Description: Heart disease is the most common ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>FORUMS</td>\n",
       "      <td>boards.4chan.org</td>\n",
       "      <td>http://boards.4chan.org/tg/thread/47745953#p47...</td>\n",
       "      <td>6/16/2016</td>\n",
       "      <td>22:24:00</td>\n",
       "      <td>6/17/2016 7:54</td>\n",
       "      <td>/40krpg/ 40K Roleplay General</td>\n",
       "      <td>&gt;&gt;47814636 You, I like you. Shame the degenera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>BLOG</td>\n",
       "      <td>http://healthtipsarticles.com</td>\n",
       "      <td>http://healthtipsarticles.com/tips-from-the-ex...</td>\n",
       "      <td>2/1/2016</td>\n",
       "      <td>0.172893519</td>\n",
       "      <td>42401.38123</td>\n",
       "      <td>Tips from the experts for a heart-healthy life...</td>\n",
       "      <td>When it comes to good advice about heart healt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>BLOG</td>\n",
       "      <td>evelynfastforward.blogspot.com</td>\n",
       "      <td>http://evelynfastforward.blogspot.com/2016/07/...</td>\n",
       "      <td>7/23/2016</td>\n",
       "      <td>14:20:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>One of my dear friends said to me one morning:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>BLOG</td>\n",
       "      <td>http://healthandfitness1blog.blogspot.com</td>\n",
       "      <td>http://healthandfitness1blog.blogspot.com/2016...</td>\n",
       "      <td>4/13/2016</td>\n",
       "      <td>6:50:00</td>\n",
       "      <td>4/13/2016 16:20</td>\n",
       "      <td>Lowering cholesterol with vegetable oils may n...</td>\n",
       "      <td>Study suggests avoiding saturated fats may not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>FORUMS</td>\n",
       "      <td>www.xboxhacker.org</td>\n",
       "      <td>http://www.xboxhacker.org/index.php?topic=7180...</td>\n",
       "      <td>6/16/2016</td>\n",
       "      <td>4:35:00</td>\n",
       "      <td>6/16/2016 14:05</td>\n",
       "      <td>Heart failure is a professional bandage.</td>\n",
       "      <td>Constipation can trick a cure before negative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>YOUTUBE</td>\n",
       "      <td>http://www.youtube.com</td>\n",
       "      <td>http://youtube.com/watch?v=bDPtIxjuzmQ</td>\n",
       "      <td>6/20/2016</td>\n",
       "      <td>10:04:17</td>\n",
       "      <td>6/20/2016 19:34</td>\n",
       "      <td>Hakan Altay, M.D. talks about ?Inflammation an...</td>\n",
       "      <td>Description: The E-Cardiology Academy, a novel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.facebook.com/permalink.php?id=10153...</td>\n",
       "      <td>2-Jun-16</td>\n",
       "      <td>4:34 PM</td>\n",
       "      <td>6/2/2016 16:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"  I got congestive heart failure and haven't ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Source                                       Host  \\\n",
       "550    FORUMS                         healthunlocked.com   \n",
       "65     FORUMS                          www.alsearsmd.com   \n",
       "143   YOUTUBE                     http://www.youtube.com   \n",
       "214    FORUMS                           boards.4chan.org   \n",
       "512      BLOG              http://healthtipsarticles.com   \n",
       "368      BLOG             evelynfastforward.blogspot.com   \n",
       "172      BLOG  http://healthandfitness1blog.blogspot.com   \n",
       "271    FORUMS                         www.xboxhacker.org   \n",
       "39    YOUTUBE                     http://www.youtube.com   \n",
       "89   Facebook                                        NaN   \n",
       "\n",
       "                                                  Link   Date(ET)  \\\n",
       "550  https://healthunlocked.com/afassociation/posts...   5/7/2016   \n",
       "65   http://www.alsearsmd.com/2016/04/better-than-a...   4/1/2016   \n",
       "143             http://youtube.com/watch?v=Nx8kpdUkqME  6/21/2016   \n",
       "214  http://boards.4chan.org/tg/thread/47745953#p47...  6/16/2016   \n",
       "512  http://healthtipsarticles.com/tips-from-the-ex...   2/1/2016   \n",
       "368  http://evelynfastforward.blogspot.com/2016/07/...  7/23/2016   \n",
       "172  http://healthandfitness1blog.blogspot.com/2016...  4/13/2016   \n",
       "271  http://www.xboxhacker.org/index.php?topic=7180...  6/16/2016   \n",
       "39              http://youtube.com/watch?v=bDPtIxjuzmQ  6/20/2016   \n",
       "89   http://www.facebook.com/permalink.php?id=10153...   2-Jun-16   \n",
       "\n",
       "        Time(ET)        time(GMT)  \\\n",
       "550  0.388888889      42497.78472   \n",
       "65      10:57:00   4/1/2016 20:27   \n",
       "143     16:46:41   6/22/2016 2:16   \n",
       "214     22:24:00   6/17/2016 7:54   \n",
       "512  0.172893519      42401.38123   \n",
       "368     14:20:00              NaN   \n",
       "172      6:50:00  4/13/2016 16:20   \n",
       "271      4:35:00  6/16/2016 14:05   \n",
       "39      10:04:17  6/20/2016 19:34   \n",
       "89       4:34 PM   6/2/2016 16:34   \n",
       "\n",
       "                                                 Title  \\\n",
       "550     HealthUnlocked | The social network for health   \n",
       "65                  Better than Aspirin for Your Heart   \n",
       "143  Heart Failure: Palliative Approaches to Care O...   \n",
       "214                      /40krpg/ 40K Roleplay General   \n",
       "512  Tips from the experts for a heart-healthy life...   \n",
       "368                                                NaN   \n",
       "172  Lowering cholesterol with vegetable oils may n...   \n",
       "271           Heart failure is a professional bandage.   \n",
       "39   Hakan Altay, M.D. talks about ?Inflammation an...   \n",
       "89                                                 NaN   \n",
       "\n",
       "                                       TRANS_CONV_TEXT  \n",
       "550  Hi all I know that there have been lots of pos...  \n",
       "65   Health Articles Better than Aspirin for Your H...  \n",
       "143  Description: Heart disease is the most common ...  \n",
       "214  >>47814636 You, I like you. Shame the degenera...  \n",
       "512  When it comes to good advice about heart healt...  \n",
       "368  One of my dear friends said to me one morning:...  \n",
       "172  Study suggests avoiding saturated fats may not...  \n",
       "271  Constipation can trick a cure before negative ...  \n",
       "39   Description: The E-Cardiology Academy, a novel...  \n",
       "89   \"  I got congestive heart failure and haven't ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly sample 10 rows from the train set\n",
    "test_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cfeb6365d0105867246d1e433c33db340265f85a"
   },
   "source": [
    "We can find out some more information about the train set and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1157 entries, 0 to 1156\n",
      "Data columns (total 9 columns):\n",
      "Source             1157 non-null object\n",
      "Host               1098 non-null object\n",
      "Link               1157 non-null object\n",
      "Date(ET)           1157 non-null object\n",
      "Time(ET)           1157 non-null object\n",
      "time(GMT)          996 non-null object\n",
      "Title              941 non-null object\n",
      "TRANS_CONV_TEXT    1156 non-null object\n",
      "Patient_Tag        1157 non-null int64\n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 81.4+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "b6a3c6ef6c0c25ce3f38849f8c421be84967e879"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 571 entries, 0 to 570\n",
      "Data columns (total 8 columns):\n",
      "Source             571 non-null object\n",
      "Host               541 non-null object\n",
      "Link               571 non-null object\n",
      "Date(ET)           571 non-null object\n",
      "Time(ET)           571 non-null object\n",
      "time(GMT)          480 non-null object\n",
      "Title              454 non-null object\n",
      "TRANS_CONV_TEXT    571 non-null object\n",
      "dtypes: object(8)\n",
      "memory usage: 35.8+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e80764008b5cce25921b969f68f09a266b9ad60f"
   },
   "source": [
    "Most of the data is non-numeric in nature. It can also be seen that both the train and test sets have missing values. Let's talk in exact numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "30b7ad22bf554aa1b34064f1c7bb30213a166483"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Source               0\n",
       "Host                59\n",
       "Link                 0\n",
       "Date(ET)             0\n",
       "Time(ET)             0\n",
       "time(GMT)          161\n",
       "Title              216\n",
       "TRANS_CONV_TEXT      1\n",
       "Patient_Tag          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "7d99d86651e8ecb7c42498ad3d2339e138a4b4dc",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Source               0\n",
       "Host                30\n",
       "Link                 0\n",
       "Date(ET)             0\n",
       "Time(ET)             0\n",
       "time(GMT)           91\n",
       "Title              117\n",
       "TRANS_CONV_TEXT      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My intuition says that the maximum information of the dataset lies in the `TRANS_CONV_TEXT` column values which will help us to determine the labels of the text correctly. We can build the baseline model just by using two columns - `TRANS_CONV_TEXT` and `Patient_Tag`. So the problem statement can now be reduced to - \n",
    "> Given a conversation text, the task is to determine if the text is patient tagged or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f51cd655c754c6c6008319792386db4949040ba8"
   },
   "source": [
    "The task is a _binary classification task_.\n",
    "\n",
    "We can see that the train set has one observation in which the value for `TRANS_CONV_TEXT` is missing. We can drop this observation otherwise, it can be problematic to create our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_uuid": "7e2056006e3826bca47c2e75fcf4b94d4056a594"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Host</th>\n",
       "      <th>Link</th>\n",
       "      <th>Date(ET)</th>\n",
       "      <th>Time(ET)</th>\n",
       "      <th>time(GMT)</th>\n",
       "      <th>Title</th>\n",
       "      <th>TRANS_CONV_TEXT</th>\n",
       "      <th>Patient_Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>FORUMS</td>\n",
       "      <td>www.reddit.com</td>\n",
       "      <td>https://www.reddit.com/r/science/comments/4ogb...</td>\n",
       "      <td>2016-06-16</td>\n",
       "      <td>19:25:00</td>\n",
       "      <td>6/17/2016 4:55</td>\n",
       "      <td>Teenage weight is linked to risk of heart fail...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Source            Host     ...     TRANS_CONV_TEXT Patient_Tag\n",
       "841  FORUMS  www.reddit.com     ...                 NaN           0\n",
       "\n",
       "[1 rows x 9 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df.TRANS_CONV_TEXT.isna()==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "c2669654e765d32b5be64b6cfecc369609a889e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Source               0\n",
       "Host                59\n",
       "Link                 0\n",
       "Date(ET)             0\n",
       "Time(ET)             0\n",
       "time(GMT)          161\n",
       "Title              216\n",
       "TRANS_CONV_TEXT      0\n",
       "Patient_Tag          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.drop(841, inplace=True)\n",
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now extract the values for the columns `TRANS_CONV_TEXT` and `Patient_Tag` and separate them out in a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "8bd61b526975b9f818b260609e235cc9772a99d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1156, 2), (571, 1))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_frame = train_df[['TRANS_CONV_TEXT', 'Patient_Tag']]\n",
    "new_test_frame = test_df[['TRANS_CONV_TEXT']]\n",
    "\n",
    "new_train_frame.shape, new_test_frame.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save these two DataFrames for our further use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_frame.to_csv('new_train_frame.csv')\n",
    "new_test_frame.to_csv('new_test_frame.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are now dealing with just two columns, it would be wise to investigate the data a bit more. Let's start by analyzing the article lengths - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     1156.000000\n",
       "mean      1851.491349\n",
       "std       2324.415684\n",
       "min          2.000000\n",
       "25%        379.750000\n",
       "50%        964.000000\n",
       "75%       2441.250000\n",
       "max      16000.000000\n",
       "Name: TRANS_CONV_TEXT, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_frame['TRANS_CONV_TEXT'].apply(len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here's the above information conveyed in words - \n",
    "* Average length of the texts - 1852 (approx)\n",
    "* Minimum length of the texts - 2\n",
    "* Highest length of the texts - 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the test set - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      571.000000\n",
       "mean      1851.010508\n",
       "std       2399.454322\n",
       "min          3.000000\n",
       "25%        391.000000\n",
       "50%        971.000000\n",
       "75%       2530.000000\n",
       "max      16000.000000\n",
       "Name: TRANS_CONV_TEXT, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test_frame['TRANS_CONV_TEXT'].apply(len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is same except for the minimum length of the texts which in this case is 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now find out the class distribution to check if there is a class-imbalance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    916\n",
       "1    240\n",
       "Name: Patient_Tag, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_frame['Patient_Tag'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there is a class-imbalance. But let's ignore this fact and proceed towards building a baseline. Let's first split the train set into train (yes another one) and validation sets in a 80:20 ratio respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(new_train_frame['TRANS_CONV_TEXT'], new_train_frame['Patient_Tag'], \\\n",
    "                                                    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model we are going to build takes numbers as input. So, we will have to transform/preprocess the data to achieve the numeric conversion needed here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Let's first remove the digits from the conversation texts which is a pretty common text-data preprocessing step. And then, let's vectorize the conversation texts which is just another way of saying finding a good numerical measure to characterize the texts. We will use count-vectorization which will help us converting the collection of conversation texts to a matrix of token (words in this case) counts. \n",
    "\n",
    "A simple reason behind going for count-based preprocessing is that we are interested in finding out the presence of specific words that denote that a conversation text is patient tagged. My hypothesis is that the order of the words can be ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import digits\n",
    "\n",
    "def remove_digits(s: str) -> str:\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    res = s.translate(remove_digits)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(remove_digits)\n",
    "X_valid = X_valid.apply(remove_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count-vectorizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "652333d343bdc685a8b26eb3aa5feaf711708478"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=None, lowercase=True,\n",
    "                             ngram_range=(1, 1), min_df=2, max_df=0.4, binary=True)\n",
    "\n",
    "train_features = vectorizer.fit_transform(X_train)\n",
    "train_labels = y_train\n",
    "\n",
    "valid_features = vectorizer.transform(X_valid)\n",
    "valid_labels = y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>A note on the hyperparameter value choices </b>-\n",
    "* `stop_words=None` - Means that we are instructing the CountVectorizer to not filter out the stop words. \n",
    "* `lowercase=True` - Converts all characters to lowercase before tokenizing.\n",
    "* `ngram_range=(1,1)` - Specifies the minimum and maximum limit on the n-gram words to be extracted. (1,1) means we are considering uni-gram. \n",
    "* `min_df=2` - When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. \n",
    "* `max_df=0.4` - When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold. If float, the parameter represents a proportion of documents, integer absolute counts. \n",
    "* `binary=True` - This means that all the non-zero counts will be set to 1, This is useful for our baseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a **Bernoulli Naive Bayes** model. We are using the Bernoulli variant of the the Naive Bayes model as our task is a binary classification task. To build the model, we would need a corpus of consisting of documents. We have already built this corpus in the previous step. Our corpus contains documents which are nothing but the conversation texts and the corpus denotes if a certain word is present in a particular document or not. \n",
    "\n",
    "So, the problem statement can now be stated more mathematically (in terms of conditional probability) - \n",
    "> What is the probability of a class given a document? \n",
    "\n",
    "It can also be expressed as - \n",
    "$P(c|d)$\n",
    "\n",
    "In our case, the class (or label) is `Patient_Tag` and the documents are the conversation text (`TRANS_CONV_TEXT`) as mentioned before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.91      0.93       179\n",
      "          1       0.73      0.89      0.80        53\n",
      "\n",
      "avg / total       0.91      0.90      0.90       232\n",
      "\n",
      "Accuracy:0.9008620689655172\n"
     ]
    }
   ],
   "source": [
    "model = BernoulliNB(fit_prior=True)\n",
    "model.fit(train_features, train_labels)\n",
    "\n",
    "valid_preds = model.predict(valid_features)\n",
    "print(classification_report(valid_labels, valid_preds))\n",
    "print(f'Accuracy:{accuracy_score(valid_labels, valid_preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Setting `fit_prior=True` means that the model will learn the class priors as well.)\n",
    "\n",
    "We can see that the model's precision is not that good when it comes to the positive classes. So there is scope of improvement in that regard. \n",
    "\n",
    "We can now train our baseline using both the train and validation set and then use it to make predictions on the actual test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sayakpaul/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_frame['TRANS_CONV_TEXT'] = new_train_frame['TRANS_CONV_TEXT'].apply(remove_digits)\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=None, lowercase=True,\n",
    "                             ngram_range=(1, 1), min_df=2, max_df = 0.4)\n",
    "\n",
    "features = vectorizer.fit_transform(new_train_frame['TRANS_CONV_TEXT'])\n",
    "labels = new_train_frame['Patient_Tag']\n",
    "\n",
    "model = BernoulliNB(fit_prior=True)\n",
    "model.fit(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions and preparing the submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have to make sure that the same preprocessing steps are applied on the test set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sayakpaul/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "new_test_frame['TRANS_CONV_TEXT'] = new_test_frame['TRANS_CONV_TEXT'].apply(remove_digits)\n",
    "\n",
    "test_features = vectorizer.transform(new_test_frame['TRANS_CONV_TEXT'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_for_submission = pd.read_csv('../dataset/test.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['Index'] = test_for_submission['Index']\n",
    "submission['Patient_Tag'] = test_preds\n",
    "\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index,Patient_Tag\r\n",
      "1,0\r\n",
      "2,1\r\n",
      "3,0\r\n",
      "4,1\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further considerations - \n",
    "\n",
    "* Trying TF-IDF based vectorization as the Count-Vectorization suffers from a problem. The common words that occur in similar frequencies in all documents (i.e., words that are not particularly unique to the text samples in the dataset) are not penalized. For example, words like a will occur very frequently in all texts. So a higher token count for the than for other more meaningful words is not very useful.\n",
    "\n",
    "* Trying out other n-gram models like shallow neural networks, logistic regression and so on. We are not going for sequence models as we are not considering the order of the words here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References - \n",
    "\n",
    "* scikit-learn official documentation - https://scikit-learn.org\n",
    "* Google Developer Guides on Machine Learning - https://developers.google.com/machine-learning/guides\n",
    "* How to Clean Text for Machine Learning with Python - https://machinelearningmastery.com/clean-text-machine-learning-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
